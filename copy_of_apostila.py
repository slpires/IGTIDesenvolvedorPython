# -*- coding: utf-8 -*-
"""Copy of apostila.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-KnYEIT6w6keCLZTDXnmIN19950Y-SuN
"""

import this

minha_string ="Disciplina Aplicações em Deep Learning"  #armazena a string

id(minha_string) #identificador do objeto

type(minha_string) #tipo de objeto

minha_string  #mostra a string armazenada

numero_inteiro= 1234 # representação de numero inteiro

type(numero_inteiro)

numero_inteiro + 10  # adição

numero_inteiro - 10 #subtração

numero_inteiro * 2  # multiplicação

numero_inteiro / 2 # divisão

s1 = "Esta é a disciplina"

s2 = " Aplicações em Deep Learning"

print(s1,s2)

print(s1+'\n'+s2)

''.join([s1,s2]) #concatena a lista de strings

s1[::-1] # percorre a string de maneira inversa

l1= ['azul','branco', 'amarelo', 'vermelho']   #exemplo de criação de listas

l2= list ([1,'coxinha',10,'pasteis',0.25e-3])  #exemplo de criação de listas

l3= [1,2,3,['a','b','c'], ['IGTI','ADL']]

print(l1,l2)

print(l3)

#acessando itens das listas
l1

l1[0]

l1[0]+' '+l1[2]

#acessando intervalos de itens 
l2[1:3]

numeros= list(range(10)) # gera uma lista contendo os numeros de 0 a 9
numeros[:]

numeros[2:5]

#concatenando listas
numeros*2

numeros+l2

#acessando listas aninhadas
l3

l3[3]

l3[4]

l3[3][1]

#adicionando elemetos à lista
l3.append('Aplicações em Deep Learning')

l3

#retirando um elemento da lista
l3.pop(3)

l3

l1 = [1,2,3,3,3,4,5,5,5,6,6,7,7,7,7,8,8] #cria uma lista com numeros repetidos

set(l1)    #cria um conjunto através do método set()

s1 = set(l1)

#verifica se um elemento pertence ao conjunto
1 in s1

10 in s1

s2 = {3,5,10,11,12}   #cria um conjunto através do { }

#operaçõs com cojuntos
s1 - s2     # diferença entre consjuntos

s1 | s2    #união de conjuntos

s1 & s2  #interseçãod e conjuntos

s1 ^ s2 # elementos que nao aparecem, ao mesmo tempo, em ambos

d1 = {'azul': 3, 'branco': 4, 'verde': 6, 'preto': 1}
d1

#recuperando itens de um dicionário
d1.get('azul')

d1['azul']

# adicionando valores
d1['amarelo']=4
d1

d1.keys()   #recuperando as chaves

d1.values()   # recuperando os valores

#cria dicionário utilizando o método dict()
d2 = dict({'laranja': 47,'roxo': 21,'azul':58})
d2

#atualizando o dicionário d1 utilizando o d2
d1.update(d2)
d1

#criando tuplas
t1= (1,)
t1

#identificação da tupla
id(t1)

# modificar o conteúdo da tupla, modifica o endereço
t1 = t1 + (2,3,4,5,6)
t1

id(t1)

# tuplas são imutáveis 
t1[2]= 1000

tupla = (['IGTI','Disciplina'],['Aplicações','Deep Learning'])
tupla[0]

l1,l2 = tupla
print(l1,l2)

variavel = 'azul'
if variavel =='azul':
  print("Variável é AZUL")

variavel = 'branco'
if variavel =='azul':
  print("Variável é AZUL")
elif variavel == 'branco':
  print('Variável é BRANCO')

variavel = 'verde'
if variavel =='azul':
  print("Variável é AZUL")
elif variavel == 'branco':
  print('Variável é BRANCO')
else:
  print('Variável é VERDE')

#ilustrando loops
sequencia = range(0,5)
for num in sequencia:
  print(num)

sum = 0
for num in sequencia:
  sum+= num
print(sum)

sequencia = range(0,5)
for num in sequencia:
  print(num)
else:
  print('loop terminou normalmente')

sequencia = range(0,5)
for num in sequencia:
  if num <3:
    print(num)
  else:
    break
else:
  print('loop terminou normalmente')

#ilustrando loop while
num = 5
while num >0:
  print(num)
  num -=1       #necessário para evitar o loop infinito

#exemplo de uma função que retorna o quadrado de um número
def retornaQuadrado(numero):
  return numero*numero

numero=5
print("O quadrado de {} é: {}".format(numero,retornaQuadrado(numero)))



"""# **Conteúdo Extra - não é abordado no fundamentos de Python**

---------------------**Análise de sentimento com Python** ----------------------
"""

#gerando as stopwords
import nltk
import numpy as np
nltk.download('stopwords')
nltk.download('punkt')
from pprint import pprint

stopWordPortugues = nltk.corpus.stopwords.words('portuguese')
print(np.transpose(stopWordPortugues))

#gerando os tokens de sentenças
sample_text=""" O menino gosta de jogar futebol aos finais de semana. 
Ele gosta de jogar com seus amigos Marcos e João, mas não gosta de brincar
com a irmã Marcela"""
tokenizacao_sentencas=nltk.sent_tokenize
sample_sentence = tokenizacao_sentencas(text=sample_text)
pprint(sample_sentence)

#tokenização de palavras
sample_sentence='O menino gosta de jogar futebol aos finais de semana.'
tokenizacao_palavras=nltk.word_tokenize
sample_words = tokenizacao_palavras(text=sample_sentence)
pprint(sample_words)

#gerando as amostras de stem
from nltk.stem import PorterStemmer
from nltk.stem import RSLPStemmer
nltk.download('rslp')

#gerado stem através do nltk
ps=PorterStemmer()
stemmer =RSLPStemmer()

print(ps.stem('jumping'))
print(stemmer.stem('amoroso')) 
print(stemmer.stem('amorosa'))
print(stemmer.stem('amados'))

from nltk.stem import SnowballStemmer   # mais indicado para a lingua portuguesa

print('Linguagens suportadas %s',SnowballStemmer.languages)

ss = SnowballStemmer("portuguese")
print(ss.stem('casado'))
print(ss.stem('casarão'))
print(ss.stem('casa'))

#instalando os pacotes necessários no google colab
!pip3 install spacy 
!python -m spacy download pt_core_news_sm

!python -m spacy link pt_core_news_sm pt  #realizado o link das bibliotecas para funcionar corretamente o idioma português

#bibliotecas que possuem um melhor desempenho para a analise do idioma português
import spacy
import spacy.cli
spacy.cli.download("pt_core_news_sm")

nlp = spacy.load("pt")

#processo de lematização em uma sentença
import spacy
from spacy.lang.pt.examples import sentences 

nlp = spacy.load('pt')      # carrega o idioma português
doc = nlp(sentences[0])     #carrega a primeira sentença existente no corpo de exemplo
print(doc)                  #imprime a sentença completa
print("Token\tPOS\tLemma")
for token in doc:           #encontra os token, POS (parts os speech).
    print(token.text, token.pos_,token.lemma_)

token.text

"""** ---------------------**Exemplo de criação do BoW (bag of words) ---------------------**** **"""

sentenca="O IGTI oferece especializacao em Deep Learning. Deep Learning e utilizado em diversas aplicacoes. As aplicacoes de deep learning sao estudadas nesta especializacao"

#coloca toda a sentença em lowercase
sentenca=sentenca.lower()

print(sentenca)

#tokenização de sentencas
tokenizacao_sentencas=nltk.sent_tokenize
sample_sentence = tokenizacao_sentencas(text=sentenca)
pprint(sample_sentence)

sample_sentence[0]

#tokenização de palavras
tokenizacao_palavras=nltk.word_tokenize
list_words=[]
for i in range(len(sample_sentence)):
  sample_words = tokenizacao_palavras(text=sample_sentence[i])
  list_words.extend(sample_words)

print(list_words)

#tokeniza palavras
def tokenizaPalavras(sentenca):
  sample_words = tokenizacao_palavras(text=sentenca)
  return sample_words


#removendo stopwords e criando o BoW
def removeStopWords(list_of_words):

  my_stop_words=['o','em','as','de','sao','nesta','.','e','a','na','do'] # cria a lista de stopwords
  list_cleaned=set(list_of_words)-set(my_stop_words)
  return list_cleaned

my_BoW=removeStopWords(list_words)

print(my_BoW)

#Cria o vetor que representa a sentenca na BoW 
def bagofwords(sentence, words):
    sentence_words = extract_words(sentence)
    # conta a frequência de palavras que estão no vetor do BoW
    bag = np.zeros(len(words))
    for sw in sentence_words:
        for i,word in enumerate(words):
            if word == sw: 
                bag[i] += 1
                
    return np.array(bag)

sentenca_teste='o igti oferece especializacao em deep learning'
print(bagofwords(sentenca_teste,my_BoW))

"""******---------------------**Processamento de imagens com Python --------------------- ****"""

import cv2  # biblioteca openCV

# Criação do objeto CascadeClassifier 
face_detection = cv2.CascadeClassifier("haarcascade_frontalface_default.xml") #arquivo xml que contém o treinamento do HaaR

# Carregando a imagem 
img = cv2.imread("CAPA2.jpg")

# Lendo a imagem em uma escala de cinza
gray_img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)

# procurando pelas coordenadas da imagem
faces = face_detection.detectMultiScale(gray_img, scaleFactor = 1.05, minNeighbors=5)

from google.colab.patches import cv2_imshow   # biblioteca necessária para o google colab mostrar a imagem
import matplotlib.pyplot as plt   # mostrar a imagem no tamanho desejado

for x,y,w,h in faces:       #identifica as coordenadas da foto e desenha o retângulo
    img = cv2.rectangle(img, (x,y), (x+w,y+h),(0,255,0),3)
 

plt.figure(figsize=(20,10))
cv2_imshow(img)
 
cv2.waitKey(0)
 
cv2.destroyAllWindows()

"""******---------------------**Criação de um Chatbot com Python --------------------- ****"""

!pip3 install chatterbot   #instalando as bibliotecas necessárias

!pip3 install chatterbot_corpus  #instalando as bibliotecas necessárias

from chatterbot.trainers import ListTrainer  #ListTrainer responsável por permitir que
                                        #uma lista de strings seja utilizada no processo de aprendizagem do Bot.
from chatterbot import ChatBot          #classe para criação do chatbot

#criaçaõ das perguntas/respostas para o bot
bot = ChatBot('ChatBot IGTI')                #objeto Chatbot

conversa = ['Oi', 'Olá', 'Tudo bem?', 'Tudo ótimo','Você gosta de programar?', 'Sim, eu programo em Python']
#foram criadas 3 perguntas/respostas. O segundo item da lista corresponde a resposta do item anterior


trainer = ListTrainer(bot) #indica que o treinamento será através da lista de perguntas/respostas
trainer.train(conversa)   #realiza o treinamento através das perguntas e respostas

#loop utilizado para capturar a interação com o usuário
while True:
    pergunta = input("Usuário: ")
    resposta = bot.get_response(pergunta)
    if float(resposta.confidence) > 0.5:   # verifica se existe alguma resposta armazenada com probabilidade 
                                          #maior que 50%  de ser a resposta à pergunta
        print('IGTI Bot: ', resposta)
    else:
        print('IGTI Bot: Ainda não sei responder esta pergunta')

